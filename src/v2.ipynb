{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.corpus import stopwords \n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn import preprocessing\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from scipy.cluster import  hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained Word2Vec and sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = api.load(\"word2vec-google-news-300\", return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(model_path, binary= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized model so we can use wmdistance later\n",
    "w2v_norm = KeyedVectors.load_word2vec_format(model_path, binary= True)\n",
    "w2v_norm.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marks and Spencers Ltd', 'M&S Limited', 'NVIDIA Ireland', 'SLOUGH SE12 2XY', '33 TIMBER YARD,LONDON, L1 8XY', '44 CHINA ROAD, KOWLOON, HONG KONG', 'XYZ 13423 / ILD', 'ABC/ICL/20891NC', 'HARDWOOD TABLE', 'PLASTIC BOTTLE', 'LONDON', 'HONG KONG', 'ASIA', 'JP Morgan & Chase Co.', 'ICNAO02312', 'TOYS', '5 Time Square, New York, NY 10036', 'COMPUTER PARTS', 'INTEL CORPORATION', 'INTEL CO', 'Ryland Group Inc.', 'Sabre Holdings Corp', 'Safeco Corp', '4CE0460D0G', 'Vero Beach, Florida', 'WINE', 'Microwave', 'Plastic container', 'Europe', 'Canada', 'HGU6UH3']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/data.txt', 'r') as f: \n",
    "    data = f.readlines()\n",
    "f.close()\n",
    "data = [x.rstrip().lstrip().strip('\\\",') for x in data]\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation.replace('#', '').replace('&', ''))\n",
    "stop_words = set(stopwords.words('english')) \n",
    "    # After trying different preprocessing combinations, I decided not to filter out stops words, \n",
    "    # and keept '#' and '&' since they can help defining a string in our specific case \n",
    "def get_tokens(x): \n",
    "    x = re.sub(r'\\d', '#', x).lower()\n",
    "    tokens = word_tokenize(x)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [x for x in tokens if x != '']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_token'] = df['text'].apply(lambda x: get_tokens(x))\n",
    "df['clean_string'] = df['clean_token'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word2vec vectors from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = w2v.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vector per string\n",
    "def get_vector(x):\n",
    "    agg_vector = []\n",
    "    for token in x: \n",
    "        if token in model_vocab: \n",
    "            v = w2v[token]  \n",
    "        elif token.capitalize() in model_vocab:\n",
    "            v = w2v[token.capitalize()] \n",
    "        else: \n",
    "            #print (token, 'NOT FOUND!')\n",
    "            v = w2v['#'] # assign unknown words (usually random letters in a serial number) to # to skew the vector\n",
    "        agg_vector.append(v)\n",
    "    agg_vector = np.array(agg_vector)\n",
    "    agg_vector = np.mean(agg_vector, axis = 0)\n",
    "    return agg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcicl NOT FOUND!\n",
      "icnao NOT FOUND!\n",
      "hgu NOT FOUND!\n"
     ]
    }
   ],
   "source": [
    "df['vecter_300'] = df['clean_token'].apply(lambda x: get_vector(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding word and character count, and a flag for possible company name\n",
    "company_str = ['corporation', 'inc', 'co', 'corp', 'cooperatives', 'ltd', 'limited', 'company', \\\n",
    "               'lp', 'incorporated', 'international']\n",
    "def get_word_cnt(x): \n",
    "    return len(x)\n",
    "\n",
    "def get_char_cnt(x): \n",
    "    return len(''.join(x))\n",
    "\n",
    "def get_company_flag(x): \n",
    "    if any(w in company_str for w in x): \n",
    "        return 1\n",
    "    else: return 0 \n",
    "        \n",
    "df['word_count'] = df['clean_token'].apply(lambda x: get_word_cnt(x))\n",
    "df['char_count'] = df['clean_token'].apply(lambda x: get_char_cnt(x))\n",
    "df['company_str'] = df['clean_token'].apply(lambda x: get_company_flag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_features(row): \n",
    "    concat_list = np.array([row['word_count'], row['char_count'], row['company_str']])\n",
    "    new_concat_list = np.append(row['vecter_300'], concat_list)\n",
    "    return new_concat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_feature_concat'] = df.apply(lambda row: concat_features(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector size:  (31, 303)\n"
     ]
    }
   ],
   "source": [
    "def scale_mat(vec_matrix,preprocess_type):\n",
    "    if preprocess_type == 'MinMax':\n",
    "        scale = preprocessing.MinMaxScaler().fit(vec_matrix)  \n",
    "    if preprocess_type == 'Standard':\n",
    "        scale = preprocessing.StandardScaler().fit(vec_matrix)\n",
    "    vals = scale.transform(vec_matrix)\n",
    "    return vals\n",
    "\n",
    "vector_matrix = np.array(df['new_feature_concat'].tolist())\n",
    "vector_matrix = scale_mat(vector_matrix, 'MinMax')\n",
    "print ('vector size: ', vector_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reduce dimensions\n",
    "sklearn_pca = PCA(n_components = 2)\n",
    "vals_pca = sklearn_pca.fit_transform(vector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 2D array to df \n",
    "df['2D'] = vals_pca.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "data2D = np.array(df['2D'].tolist())\n",
    "n = df.index\n",
    "plt.scatter(vals_pca[:,0], vals_pca[:,1])\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (vals_pca[i,0], vals_pca[i,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some sort of patterns forming. Some examples are: \n",
    "<br> On far right there are locations: 10(LONDON), 12(ASIA), 28(Europ), and 29(Canada) \n",
    "<br> The center cluster is mostly companies: 0(Marks and Spencers Ltd), 1(M&S Limited), 21(Sabre Holdings Corp), 13(JP Morgan & Chase Co)\n",
    "<br> The top part is mostly items, there are 8(HARDWOOD TABLE), 9(PLASTIC BOTTLE), 15(TOYS), 17(COMPUTER PARTS), 25(WINE), 26(microwave), 27(Plastic container)\n",
    "<br> On bottom left there are serial numbers: 6(XYZ 13423 / ILD), 7(ABC/ICL/20891NC), 14(ICNAO02312), 23(4CE0460D0G), 30(HGU6UH3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curious to see the clusters in a more mathematical way\n",
    "<b> (Though hierarchy can't really be applied to massive data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(df, cluster_col):\n",
    "    cluster_dict = df.groupby(cluster_col)['text'].apply(list).to_dict()\n",
    "    for k, v, in cluster_dict.items():\n",
    "        print (k, v)\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = hierarchy.linkage(vals_pca, method = \"average\", metric = 'euclidean')\n",
    "plt.figure()\n",
    "dn = hierarchy.dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decent hierarchy graph, looks like 1.5 would be a good threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.5\n",
    "C = hierarchy.fcluster(Z, threshold, criterion=\"distance\")\n",
    "labels = (C).tolist()\n",
    "labels = ['cluster_'+str(s) for s in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_1 ['LONDON', 'ASIA', 'Europe', 'Canada']\n",
      "\n",
      "cluster_2 ['NVIDIA Ireland']\n",
      "\n",
      "cluster_3 ['SLOUGH SE12 2XY', '33 TIMBER YARD,LONDON, L1 8XY', 'XYZ 13423 / ILD', 'ABC/ICL/20891NC', 'ICNAO02312', '5 Time Square, New York, NY 10036', '4CE0460D0G', 'HGU6UH3']\n",
      "\n",
      "cluster_4 ['Marks and Spencers Ltd', 'M&S Limited', '44 CHINA ROAD, KOWLOON, HONG KONG', 'HONG KONG', 'JP Morgan & Chase Co.', 'INTEL CORPORATION', 'INTEL CO', 'Ryland Group Inc.', 'Sabre Holdings Corp', 'Safeco Corp', 'Vero Beach, Florida']\n",
      "\n",
      "cluster_5 ['HARDWOOD TABLE', 'PLASTIC BOTTLE', 'COMPUTER PARTS', 'WINE', 'Microwave', 'Plastic container']\n",
      "\n",
      "cluster_6 ['TOYS']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['hierarchy_label'] = labels\n",
    "print_clusters(df, 'hierarchy_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceeding with these features for the second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique entity recognition (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as D\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_excel('../data/Book1.xlsx') # test string INTEL CORPORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str_1</th>\n",
       "      <th>str_2</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Marks and Spencers Ltd</td>\n",
       "      <td>M&amp;S Limited</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LONDON</td>\n",
       "      <td>London, United Kingdom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>VERO BEACH, FLORIDA</td>\n",
       "      <td>Vero Beach, FL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5 Time Sqaure, New York, NY 10036</td>\n",
       "      <td>5 Time Sqaure, NY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Pepsi Co.</td>\n",
       "      <td>Pepsico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>LONDON</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>LA, California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>ICNAO02312</td>\n",
       "      <td>IHB72910BU</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Bethlehem, Pennsylvania</td>\n",
       "      <td>Bethlehem, PA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Ernst &amp; Young LLP</td>\n",
       "      <td>EY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>cell phone</td>\n",
       "      <td>mobile phone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>9012VU/38O</td>\n",
       "      <td>OIK36121KJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 str_1                   str_2  Y\n",
       "8               Marks and Spencers Ltd             M&S Limited  1\n",
       "18                              LONDON  London, United Kingdom  1\n",
       "41                 VERO BEACH, FLORIDA          Vero Beach, FL  1\n",
       "55   5 Time Sqaure, New York, NY 10036       5 Time Sqaure, NY  1\n",
       "68                          Pepsi Co.                  Pepsico  1\n",
       "86                              LONDON              London, UK  1\n",
       "111                    Los Angeles, CA          LA, California  1\n",
       "124           United States of America                     USA  1\n",
       "149                         ICNAO02312              IHB72910BU  1\n",
       "151            Bethlehem, Pennsylvania           Bethlehem, PA  1\n",
       "169                  Ernst & Young LLP                      EY  1\n",
       "186              Boston, Massachusetts              Boston, MA  1\n",
       "194                         cell phone            mobile phone  1\n",
       "201                         9012VU/38O              OIK36121KJ  1"
      ]
     },
     "execution_count": 1330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled[labeled.Y==1] # I created some examples of equivalent entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 3)"
      ]
     },
     "execution_count": 1331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess - input dataset with str_1 and str_2\n",
    "def concat_features(row, col): \n",
    "    concat_list = np.array([row[col+'_word_count'], row[col+'_char_count'], row[col+'_company_flag']])\n",
    "    new_concat_list = np.append(row[col+'_vector_300'], concat_list)\n",
    "    return new_concat_list\n",
    "\n",
    "# return a list of characteristics features, input data with str_1 and str2 \n",
    "def get_char_features_labled(updated_df): \n",
    "    return_list = []\n",
    "    for idx, row in updated_df.iterrows(): \n",
    "        append_list = []\n",
    "        append_list.append(row['str_1_word_count']-row['str_2_word_count']) # word count difference\n",
    "        append_list.append(row['str_1_char_count']-row['str_2_char_count']) # character count difference\n",
    "        append_list.append(len(set(row['str_1_token']).intersection(set(row['str_2_token'])))) # common word\n",
    "        append_list.append(fuzz.ratio(row['str_1_clean_string'], row['str_2_clean_string'])) # fuzz ratio\n",
    "        append_list.append(fuzz.partial_ratio(row['str_1_clean_string'], row['str_2_clean_string'])) # fuzz partical\n",
    "        append_list.append(fuzz.token_set_ratio(row['str_1_clean_string'], row['str_2_clean_string'])) # fuzz token set\n",
    "        append_list.append(fuzz.partial_token_sort_ratio(row['str_1_clean_string'], row['str_2_clean_string']))\n",
    "        append_list.append(fuzz.token_sort_ratio(row['str_1_clean_string'], row['str_2_clean_string']))\n",
    "        return_list.append(append_list)\n",
    "    return return_list\n",
    "\n",
    "# return a list of distance features, input data with str_1 and str2 after normalization \n",
    "def get_dist_features_labeled(updated_df):\n",
    "    v1 = np.array(updated_df['str_1_feature_concat'].tolist())\n",
    "    v2 = np.array(updated_df['str_2_feature_concat'].tolist())\n",
    "    return_list = []\n",
    "    for i in range (v1.shape[0]): \n",
    "        append_list = []\n",
    "        if v1[i].sum()==0 or v2[i].sum()==0: \n",
    "            print (updated_df.iat[i,0])\n",
    "        append_list.append(D.cosine(v1[i], v2[i]))\n",
    "        append_list.append(D.euclidean(v1[i], v2[i]))\n",
    "        append_list.append(D.cityblock(v1[i], v2[i]))\n",
    "        append_list.append(D.minkowski(v1[i], v2[i]))\n",
    "        append_list.append(D.braycurtis(v1[i], v2[i]))\n",
    "        append_list.append(D.canberra(v1[i], v2[i]))\n",
    "        append_list.append(D.chebyshev(v1[i], v2[i]))\n",
    "        return_list.append(append_list)\n",
    "    return return_list\n",
    "\n",
    "# concat and normlize X \n",
    "def process_X(c_v, d_v): \n",
    "    return_list = []\n",
    "    for i in range(len(c_v)): \n",
    "        return_list.append(c_v[i]+d_v[i])\n",
    "    mat = np.array(return_list)\n",
    "    mat = scale_mat(mat, 'MinMax')\n",
    "    return mat\n",
    "\n",
    "def process_df(labeled_df):\n",
    "    for col in ['str_1', 'str_2']: \n",
    "        labeled_df[col+'_token'] = labeled_df[col].apply(lambda x: get_tokens(x))\n",
    "        labeled_df[col+'_clean_string'] = labeled_df[col+'_token'].apply(lambda x: ' '.join(x))\n",
    "        labeled_df[col+'_vector_300'] = labeled_df[col+'_token'].apply(lambda x: get_vector(x))\n",
    "        labeled_df[col+'_word_count'] = labeled_df[col+'_token'].apply(lambda x: get_word_cnt(x))\n",
    "        labeled_df[col+'_char_count'] = labeled_df[col+'_token'].apply(lambda x: get_char_cnt(x))\n",
    "        labeled_df[col+'_company_flag'] = labeled_df[col+'_token'].apply(lambda x: get_company_flag(x))\n",
    "        labeled_df[col+'_feature_concat'] = labeled_df.apply(lambda row: concat_features(row, col), axis = 1)\n",
    "    char_features = get_char_features_labled(labeled_df)\n",
    "    dist_features = get_dist_features_labeled(labeled_df)\n",
    "    # Got features, now concat and normalize\n",
    "    labeled_df['X'] = process_X(char_features, dist_features).tolist()\n",
    "    return labeled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 1342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a simple SVM \n",
    "labeled_df = process_df(labeled)\n",
    "X_train = np.array(labeled_df['X'].tolist())\n",
    "Y_train = labeled_df['Y'].values\n",
    "clf = svm.SVC(probability = True)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the classifier with new strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current data points \n",
    "data_inventory = labeled['str_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING_STR_TRUE: There should be an equivalent entity in the data inventory \n",
    "TESTING_STR_TRUE = ['INTEL CO', 'IBM Corporation', 'SF, California', 'Shanghai, China', \\\n",
    "                    'Charlotte, North Carolina', 'Chicago, IL', 'Washington, District of Columbia', \\\n",
    "                    'Seoul, SK', 'GYU671A3']\n",
    "# TESTING_STR_FALSE: They should NOT be groupped with any entity \n",
    "TESTING_STR_FALSE = ['Apple Inc.', 'Microsoft Corporation', 'Palm Beach, FL', 'Toronto, Canada', \\\n",
    "                     'Austin, TX', 'PwC LLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTEL CO : ['Pepsi Co. ', 'INTEL CORPORATION']\n",
      "IBM Corporation : ['INTEL CORPORATION', 'IBM Corp.']\n",
      "SF, California : ['VERO BEACH, FLORIDA', 'Los Angeles, CA', 'INTEL CORPORATION', 'San Francisco, CA', 'Charlotte, NC', 'Chicago, Illinois']\n",
      "Shanghai, China : ['San Francisco, CA', 'Shanghai, CN']\n",
      "Charlotte, North Carolina : ['Charlotte, NC']\n",
      "Chicago, IL : ['Chicago, Illinois']\n",
      "Washington, District of Columbia : ['United States of America', 'Washington, D.C.']\n",
      "Seoul, SK : ['Seoul, South Korea']\n",
      "GYU671A3 : ['5 Time Sqaure, New York, NY 10036', 'ICNAO02312', '9012VU/38O']\n"
     ]
    }
   ],
   "source": [
    "for testing_str in TESTING_STR_TRUE: \n",
    "    new_df = pd.DataFrame({'str_1': data_inventory.tolist()})\n",
    "    new_df['str_2'] = testing_str\n",
    "    new_df = process_df(new_df)\n",
    "    X_test = np.array(new_df['X'].tolist())\n",
    "    P = clf.predict(X_test)\n",
    "    new_df['clf'] = P.tolist()\n",
    "    entity_list = new_df[new_df.clf==1]['str_1'].tolist()\n",
    "    print (testing_str, ':', entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. : ['Pepsi Co. ', 'Los Angeles, CA', 'RYLAND GROUP INC.', 'San Francisco, CA', 'Charlotte, NC']\n",
      "Microsoft Corporation : ['INTEL CORPORATION']\n",
      "Palm Beach, FL : ['VERO BEACH, FLORIDA']\n",
      "Toronto, Canada : ['LONDON', 'VERO BEACH, FLORIDA', 'Los Angeles, CA', 'INTEL CORPORATION', 'San Francisco, CA', 'Boston, Massachusetts', 'Charlotte, NC', 'Chicago, Illinois', 'Washington, D.C.', 'Seoul, South Korea']\n",
      "Austin, TX : ['ASIA', 'INTEL CORPORATION', 'Boston, Massachusetts', 'IBM Corp.', 'Charlotte, NC', 'Chicago, Illinois', 'Washington, D.C.']\n",
      "PwC LLP : ['ICNAO02312', 'Ernst & Young LLP', 'cell phone']\n"
     ]
    }
   ],
   "source": [
    "for testing_str in TESTING_STR_FALSE: \n",
    "    new_df = pd.DataFrame({'str_1': data_inventory.tolist()})\n",
    "    new_df['str_2'] = testing_str\n",
    "    new_df = process_df(new_df)\n",
    "    X_test = np.array(new_df['X'].tolist())\n",
    "    P = clf.predict(X_test)\n",
    "    new_df['clf'] = P.tolist()\n",
    "    entity_list = new_df[new_df.clf==1]['str_1'].tolist()\n",
    "    print (testing_str, ':', entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp - Preprocess and get vector features (303,), return dictionary \n",
    "def get_feature_vector_303(string_input): \n",
    "    new_string_dict = { i : '' for i in df.columns}\n",
    "    token = get_tokens(string_input)\n",
    "    word_vec_google = get_vector(token)\n",
    "    word_cnt = get_word_cnt(token)\n",
    "    char_cnt = get_char_cnt(token)\n",
    "    com_flag = get_company_flag(token)\n",
    "    extra_features = np.array([word_cnt, char_cnt, com_flag])\n",
    "    v = np.append(word_vec_google, extra_features)\n",
    "    \n",
    "    new_string_dict['text'] = string_input\n",
    "    new_string_dict['clean_token'] = token\n",
    "    new_string_dict['clean_string'] = ' '.join(token)\n",
    "    new_string_dict['vecter_300'] = word_vec_google\n",
    "    new_string_dict['word_count'] = word_cnt\n",
    "    new_string_dict['char_count'] = char_cnt\n",
    "    new_string_dict['company_str'] = com_flag\n",
    "    new_string_dict['new_feature_concat'] = v\n",
    "    #v = scale_mat(v.reshape(-1,1),'MinMax')\n",
    "    return new_string_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_for_df(df, scale_type): \n",
    "    df_array = np.array(df['new_feature_concat'].tolist())\n",
    "    df_array = scale_mat(df_array,scale_type)\n",
    "    return df_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>vecter_300</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>company_str</th>\n",
       "      <th>new_feature_concat</th>\n",
       "      <th>2D</th>\n",
       "      <th>clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marks and Spencers Ltd</td>\n",
       "      <td>[marks, and, spencers, ltd]</td>\n",
       "      <td>[-0.081870556, -0.028320312, -0.05102539, 0.10...</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.08187055587768555, -0.0283203125, -0.05102...</td>\n",
       "      <td>[-0.3339800956502091, -0.35841218689219334]</td>\n",
       "      <td>marks and spencers ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M&amp;S Limited</td>\n",
       "      <td>[m, &amp;, s, limited]</td>\n",
       "      <td>[-0.15039062, 0.09075928, -0.11743164, 0.13623...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.150390625, 0.09075927734375, -0.1174316406...</td>\n",
       "      <td>[-0.5162413962301977, -0.026243635885206534]</td>\n",
       "      <td>m &amp; s limited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text                  clean_token  \\\n",
       "0  Marks and Spencers Ltd  [marks, and, spencers, ltd]   \n",
       "1             M&S Limited           [m, &, s, limited]   \n",
       "\n",
       "                                          vecter_300  word_count  char_count  \\\n",
       "0  [-0.081870556, -0.028320312, -0.05102539, 0.10...           4          19   \n",
       "1  [-0.15039062, 0.09075928, -0.11743164, 0.13623...           4          10   \n",
       "\n",
       "   company_str                                 new_feature_concat  \\\n",
       "0            1  [-0.08187055587768555, -0.0283203125, -0.05102...   \n",
       "1            1  [-0.150390625, 0.09075927734375, -0.1174316406...   \n",
       "\n",
       "                                             2D            clean_string  \n",
       "0   [-0.3339800956502091, -0.35841218689219334]  marks and spencers ltd  \n",
       "1  [-0.5162413962301977, -0.026243635885206534]           m & s limited  "
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'London United Kingdom'\n",
    "test_dict = get_feature_vector_303(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(test_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a list of characteristics features\n",
    "def get_char_features(updated_df, new_string_dict): \n",
    "    return_list = []\n",
    "    for idx, row in updated_df.iterrows(): \n",
    "        append_list = []\n",
    "        append_list.append(row['word_count']-new_string_dict['word_count']) # word count difference\n",
    "        append_list.append(row['char_count']-new_string_dict['char_count']) # character count difference\n",
    "        append_list.append(len(set(row['clean_token']).intersection(set(new_string_dict['clean_token'])))) # common word\n",
    "        append_list.append(fuzz.ratio(row['clean_string'], new_string_dict['clean_string'])) # fuzz ratio\n",
    "        append_list.append(fuzz.partial_ratio(row['clean_string'], new_string_dict['clean_string'])) # fuzz partical\n",
    "        append_list.append(fuzz.token_set_ratio(row['clean_string'], new_string_dict['clean_string'])) # fuzz token set\n",
    "        return_list.append(append_list)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_matrix = np.array(df['new_feature_concat'].tolist())\n",
    "vector_matrix = scale_mat(vector_matrix, 'MinMax')\n",
    "vals_pca = sklearn_pca.fit_transform(vector_matrix)\n",
    "df['2D'] = vals_pca.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return s list of distance features\n",
    "# input df after appeding the new string because need to do PCA \n",
    "def get_dist_features(updated_df): \n",
    "    new_vec = updated_df.tail(1)['2D'].to_numpy()[0]\n",
    "    old_vec = updated_df['2D']\n",
    "    return_list = []\n",
    "    for v in old_vec: \n",
    "        #print (updated_df.iat[i,0], i)\n",
    "        v = np.array(v)\n",
    "        append_list = []\n",
    "        append_list.append(D.cosine(v, new_vec))\n",
    "        append_list.append(D.euclidean(v, new_vec))\n",
    "        append_list.append(D.cityblock(v, new_vec))\n",
    "        append_list.append(D.minkowski(v, new_vec))\n",
    "        return_list.append(append_list)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_v = get_char_features(df, test_dict)\n",
    "dist_v = get_dist_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat and normlize X \n",
    "def process_X(c_v, d_v): \n",
    "    return_list = []\n",
    "    for i in range(len(c_v)): \n",
    "        return_list.append(c_v[i]+d_v[i])\n",
    "    mat = np.array(return_list)\n",
    "    mat = scale_mat(mat, 'MinMax')\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update training df\n",
    "train_df = pd.DataFrame(columns = ['str_1', 'str_2', 'X', 'Y'])\n",
    "train_df['str_1'] = df['text']\n",
    "train_df['str_2'] = test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_X(char_v, dist_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34, 9), (34, 4))"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['X'] = X.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label data\n",
    "train_df['Y'] = 0\n",
    "train_df.iat[10,3] = 1\n",
    "train_df.iat[33,3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = pd.concat([train_df_1, train_df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(all_train['X'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10)"
      ]
     },
     "execution_count": 1088,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 1090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 5)"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 10)\n"
     ]
    }
   ],
   "source": [
    "# try out the classifier \n",
    "try_string = 'vb, florida'\n",
    "try_dict = get_feature_vector_303(try_string)\n",
    "try_df = df.append(try_dict, ignore_index=True)\n",
    "\n",
    "#vector_matrix = np.array(try_df['new_feature_concat'].tolist())\n",
    "#vector_matrix = scale_mat(vector_matrix, 'MinMax')\n",
    "vector_matrix = scaler_for_df(try_df, 'MinMax')\n",
    "vals_pca = sklearn_pca.fit_transform(vector_matrix)\n",
    "\n",
    "try_df['2D'] = vals_pca.tolist()\n",
    "try_char_v = get_char_features(try_df, test_dict)\n",
    "try_dist_v = get_dist_features(try_df)\n",
    "try_X = process_X(try_char_v, try_dist_v)\n",
    "\n",
    "print (try_X.shape)\n",
    "\n",
    "new_train = pd.DataFrame()\n",
    "new_train['str_1'] = try_df['text']\n",
    "new_train['str_2'] = try_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = clf.predict(try_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train['clf'] = P.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor idx, row in my_df.iterrows(): \\n    my_dict = row.to_dict()\\n    character_list = []\\n    character_list.append(my_dict['word_cnt']-test_w_cnt) \\n    character_list.append(my_dict['char_cnt']-test_c_cnt) \\n    character_list.append(len(set(my_dict['clean_token']).intersection(set(test_token)))) # number of common word\\n    character_list.append(fuzz.ratio(my_dict['clean_string'], test_clean_string)) # fuzz ratio\\n    character_list.append(fuzz.partial_ratio(my_dict['clean_string'], test_clean_string)) # fuzz partical\\n    character_list.append(fuzz.token_set_ratio(my_dict['clean_string'], test_clean_string))\\n    character_list.append(fuzz.token_sort_ratio(my_dict['clean_string'], test_clean_string))\\n    character_list.append(fuzz.partial_token_set_ratio(my_dict['clean_string'], test_clean_string))\\n    character_list.append(fuzz.partial_token_sort_ratio(my_dict['clean_string'], test_clean_string))\\n    print (character_list)\\n\""
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for idx, row in my_df.iterrows(): \n",
    "    my_dict = row.to_dict()\n",
    "    character_list = []\n",
    "    character_list.append(my_dict['word_cnt']-test_w_cnt) \n",
    "    character_list.append(my_dict['char_cnt']-test_c_cnt) \n",
    "    character_list.append(len(set(my_dict['clean_token']).intersection(set(test_token)))) # number of common word\n",
    "    character_list.append(fuzz.ratio(my_dict['clean_string'], test_clean_string)) # fuzz ratio\n",
    "    character_list.append(fuzz.partial_ratio(my_dict['clean_string'], test_clean_string)) # fuzz partical\n",
    "    character_list.append(fuzz.token_set_ratio(my_dict['clean_string'], test_clean_string))\n",
    "    character_list.append(fuzz.token_sort_ratio(my_dict['clean_string'], test_clean_string))\n",
    "    character_list.append(fuzz.partial_token_set_ratio(my_dict['clean_string'], test_clean_string))\n",
    "    character_list.append(fuzz.partial_token_sort_ratio(my_dict['clean_string'], test_clean_string))\n",
    "    print (character_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a matrix of all kinds of distances \n",
    "def get_all_dist(v1, v2): \n",
    "    dist_list = []\n",
    "    dist_list.append(D.cosine(v1, v2))\n",
    "    dist_list.append(D.euclidean((v1, v2)))\n",
    "    return np.array(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.018331223903671368"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cosine_dist(init_vector, test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = df.iloc[0,3]\n",
    "b = df.iloc[1,3]\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627565141838561"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "result = 1-spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (w2v.distance('corporation', 'co'))\n",
    "print (w2v.wmdistance('corporation', 'co'))\n",
    "print (w2v_norm.distance('corporation', 'co'))\n",
    "print (w2v_norm.wmdistance('corporation', 'co'))\n",
    "'''\n",
    "def compute_similarity(s1, s2):\n",
    "    return 1.0 - (0.01 * max(fuzz.ratio(s1, s2),fuzz.token_sort_ratio(s1, s2),fuzz.token_set_ratio(s1, s2)))\n",
    "compute_similarity('INTELCORPORATION', 'INTELCO')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add distance \n",
    "def get_distance(v1, v2): \n",
    "    return w2v.distance(a, b)\n",
    "\n",
    "# add Word Mover's Distance\n",
    "def get_WMD(v1, v2): \n",
    "    return w2v_norm.wmdistance(a, b)\n",
    "\n",
    "def get_word_delta(tokens_1, tokens_2): \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_metric(a, b):\n",
    "    sim = np.isnan(dot(a, b)/(norm(a)*norm(b)))\n",
    "    if sim == False:\n",
    "        return dot(a, b)/(norm(a)*norm(b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
